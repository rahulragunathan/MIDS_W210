{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import movie files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_file = 'movies2010_2016.csv'\n",
    "plot_file = 'plots2010_2016.csv'\n",
    "actor_file = 'actors2010_2016.csv'\n",
    "director_file = 'directors2010_2016.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(movie_file)\n",
    "plot_df = pd.read_csv(plot_file)\n",
    "actor_df = pd.read_csv(actor_file)\n",
    "director_df = pd.read_csv(director_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print movie_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print plot_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print plot_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print actor_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print actor_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print director_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print director_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_list_df = movie_df.merge(plot_df, on=[u'site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace nulls in text columns with empty string (otherwise sometimes causes error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_list_df['based_on'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['cinematography'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['country'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['director'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['distributor'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['editor'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['language'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['language'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['music'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['narration'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['producer'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['screenplay'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['starring'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['story'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['written_by'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['studio'].replace(to_replace=np.nan, value=\"\", inplace=True)\n",
    "movie_list_df['plot'].replace(to_replace=np.nan, value=\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revenue_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    try:\n",
    "        revenue = float(row['revenues'])\n",
    "    except:\n",
    "        revenue = np.nan\n",
    "    revenue_arr.append(revenue)\n",
    "\n",
    "movie_list_df['revenues_clean'] = revenue_arr\n",
    "print movie_list_df['revenues_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    try:\n",
    "        length = re.sub(\"[^0-9]+\", \"\", row['length'])\n",
    "        length = int(length)\n",
    "    except:\n",
    "        length = np.nan\n",
    "    length_arr.append(length)\n",
    "\n",
    "movie_list_df['length_clean'] = length_arr\n",
    "print movie_list_df['length_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    try:\n",
    "        cost = float(row['costs'])\n",
    "    except:\n",
    "        cost = np.nan\n",
    "    cost_arr.append(cost)\n",
    "\n",
    "movie_list_df['costs_clean'] = cost_arr\n",
    "print movie_list_df['costs_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add release week and day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_arr = []\n",
    "day_of_week_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    try:\n",
    "        release_year = int(row['release_year'])\n",
    "        release_month = int(row['release_month'])\n",
    "        release_day = int(row['release_day'].split(\"-\")[0])\n",
    "        release_week = datetime.date(release_year, release_month, release_day).isocalendar()[1]\n",
    "        release_day_of_week = datetime.datetime.weekday(datetime.datetime.strptime(str(release_year)+\"-\"+str(release_month)+\"-\"+str(release_day), \"%Y-%m-%d\"))\n",
    "    except:\n",
    "        release_week = np.nan   \n",
    "        release_day_of_week = np.nan\n",
    "    day_of_week_arr.append(release_day_of_week)\n",
    "    week_arr.append(release_week)\n",
    "\n",
    "movie_list_df['release_week'] = week_arr\n",
    "movie_list_df['release_day_of_week'] = day_of_week_arr\n",
    "\n",
    "print movie_list_df[['release_week', 'release_day_of_week']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine writing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_list_df['writer'] = movie_list_df['screenplay'] + movie_list_df['written_by']\n",
    "print movie_list_df['writer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine if adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_list_df['adaptation'] = (movie_list_df['based_on'] == \"\").astype(int)\n",
    "print movie_list_df['adaptation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cast and crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_actor = 20\n",
    "actor_prefix = \"actor_\"\n",
    "actor_col_arr = [actor_prefix + str(i) for i in range(no_top_actor)]\n",
    "actor_arr = []\n",
    "actor_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    actor_list = filter(None, row['starring'].split(\"\\r\"))[:no_top_actor]\n",
    "    actor_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in actor_list]\n",
    "    actor_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in actor_list]\n",
    "    for actor in actor_list:\n",
    "        actor_master_list.add(actor)\n",
    "    actor_list_len = len(actor_list)\n",
    "    if actor_list_len < no_top_actor:\n",
    "        for i in range(no_top_actor):\n",
    "            if i >= actor_list_len:\n",
    "                actor_list.append(\"\") \n",
    "    actor_arr.append(actor_list)\n",
    "\n",
    "actor_arr_tp = np.transpose(actor_arr)\n",
    "\n",
    "for item in enumerate(actor_col_arr):\n",
    "    movie_list_df[item[1]] = actor_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[actor_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_dummy_dup_df = pd.get_dummies(movie_list_df[actor_col_arr])\n",
    "actor_dummy_dup_col_arr = list(actor_dummy_dup_df.columns.values)\n",
    "\n",
    "print actor_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_actor in actor_master_list:\n",
    "    unq_actor_col_nme = actor_prefix + unq_actor\n",
    "\n",
    "    unq_actor_col_arr = [actor_dup_dummy for actor_dup_dummy in actor_dummy_dup_col_arr if unq_actor in actor_dup_dummy]\n",
    "    actor_dummy_df[unq_actor_col_nme] = actor_dummy_dup_df[unq_actor_col_arr].sum(axis=1)\n",
    "\n",
    "actor_dummy_col_arr = list(actor_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(actor_dummy_df)\n",
    "print movie_list_df[actor_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_actor = 'Chlo'\n",
    "test_actor = 'Liam Neeson'\n",
    "test_actor_dup_col_arr = [actor_dup_dummy for actor_dup_dummy in actor_dummy_dup_col_arr if test_actor in actor_dup_dummy]\n",
    "test_actor_col_arr = [actor_dummy for actor_dummy in actor_dummy_col_arr if test_actor in actor_dummy]\n",
    "print test_actor_dup_col_arr\n",
    "print test_actor_col_arr\n",
    "\n",
    "print actor_dummy_dup_df[test_actor_dup_col_arr]\n",
    "print actor_dummy_df[test_actor_col_arr]\n",
    "\n",
    "print [sum(actor_dummy_dup_df[item]) for item in test_actor_dup_col_arr]\n",
    "print np.sum(actor_dummy_df[test_actor_col_arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cinematographer\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_cinematographer = 1\n",
    "cinematographer_prefix = \"cinematographer_\"\n",
    "cinematographer_col_arr = [cinematographer_prefix + str(i) for i in range(no_top_cinematographer)]\n",
    "cinematographer_arr = []\n",
    "cinematographer_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    cinematographer_list = filter(None, row['cinematography'].split(\"\\r\"))[:no_top_cinematographer]\n",
    "    cinematographer_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in cinematographer_list]\n",
    "    cinematographer_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in cinematographer_list]\n",
    "    for cinematographer in cinematographer_list:\n",
    "        cinematographer_master_list.add(cinematographer)\n",
    "    cinematographer_list_len = len(cinematographer_list)\n",
    "    if cinematographer_list_len < no_top_cinematographer:\n",
    "        for i in range(no_top_cinematographer):\n",
    "            if i >= cinematographer_list_len:\n",
    "                cinematographer_list.append(\"\") \n",
    "    cinematographer_arr.append(cinematographer_list)\n",
    "\n",
    "cinematographer_arr_tp = np.transpose(cinematographer_arr)\n",
    "\n",
    "for item in enumerate(cinematographer_col_arr):\n",
    "    movie_list_df[item[1]] = cinematographer_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[cinematographer_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cinematographer_dummy_dup_df = pd.get_dummies(movie_list_df[cinematographer_col_arr])\n",
    "cinematographer_dummy_dup_col_arr = list(cinematographer_dummy_dup_df.columns.values)\n",
    "\n",
    "print cinematographer_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cinematographer_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_cinematographer in cinematographer_master_list:\n",
    "    unq_cinematographer_col_nme = cinematographer_prefix + unq_cinematographer\n",
    "\n",
    "    unq_cinematographer_col_arr = [cinematographer_dup_dummy for cinematographer_dup_dummy in cinematographer_dummy_dup_col_arr if unq_cinematographer in cinematographer_dup_dummy]\n",
    "    cinematographer_dummy_df[unq_cinematographer_col_nme] = cinematographer_dummy_dup_df[unq_cinematographer_col_arr].sum(axis=1)\n",
    "\n",
    "cinematographer_dummy_col_arr = list(cinematographer_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(cinematographer_dummy_df)\n",
    "print movie_list_df[cinematographer_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Director\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_director = 2\n",
    "director_prefix = \"director_\"\n",
    "director_col_arr = [director_prefix + str(i) for i in range(no_top_director)]\n",
    "director_arr = []\n",
    "director_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    director_list = filter(None, row['director'].split(\"\\r\"))[:no_top_director]\n",
    "    director_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in director_list]\n",
    "    director_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in director_list]\n",
    "    for director in director_list:\n",
    "        director_master_list.add(director)\n",
    "    director_list_len = len(director_list)\n",
    "    if director_list_len < no_top_director:\n",
    "        for i in range(no_top_director):\n",
    "            if i >= director_list_len:\n",
    "                director_list.append(\"\") \n",
    "    director_arr.append(director_list)\n",
    "\n",
    "director_arr_tp = np.transpose(director_arr)\n",
    "\n",
    "for item in enumerate(director_col_arr):\n",
    "    movie_list_df[item[1]] = director_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[director_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "director_dummy_dup_df = pd.get_dummies(movie_list_df[director_col_arr])\n",
    "director_dummy_dup_col_arr = list(director_dummy_dup_df.columns.values)\n",
    "\n",
    "print director_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "director_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_director in director_master_list:\n",
    "    unq_director_col_nme = director_prefix + unq_director\n",
    "\n",
    "    unq_director_col_arr = [director_dup_dummy for director_dup_dummy in director_dummy_dup_col_arr if unq_director in director_dup_dummy]\n",
    "    director_dummy_df[unq_director_col_nme] = director_dummy_dup_df[unq_director_col_arr].sum(axis=1)\n",
    "\n",
    "director_dummy_col_arr = list(director_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(director_dummy_df)\n",
    "print movie_list_df[director_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributor\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_distributor = 1\n",
    "distributor_prefix = \"distributor_\"\n",
    "distributor_col_arr = [distributor_prefix + str(i) for i in range(no_top_distributor)]\n",
    "distributor_arr = []\n",
    "distributor_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    distributor_list = filter(None, row['distributor'].split(\"\\r\"))[:no_top_distributor]\n",
    "    distributor_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in distributor_list]\n",
    "    distributor_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in distributor_list]\n",
    "    for distributor in distributor_list:\n",
    "        distributor_master_list.add(distributor)\n",
    "    distributor_list_len = len(distributor_list)\n",
    "    if distributor_list_len < no_top_distributor:\n",
    "        for i in range(no_top_distributor):\n",
    "            if i >= distributor_list_len:\n",
    "                distributor_list.append(\"\") \n",
    "    distributor_arr.append(distributor_list)\n",
    "\n",
    "distributor_arr_tp = np.transpose(distributor_arr)\n",
    "\n",
    "for item in enumerate(distributor_col_arr):\n",
    "    movie_list_df[item[1]] = distributor_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[distributor_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distributor_dummy_dup_df = pd.get_dummies(movie_list_df[distributor_col_arr])\n",
    "distributor_dummy_dup_col_arr = list(distributor_dummy_dup_df.columns.values)\n",
    "\n",
    "print distributor_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distributor_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_distributor in distributor_master_list:\n",
    "    unq_distributor_col_nme = distributor_prefix + unq_distributor\n",
    "\n",
    "    unq_distributor_col_arr = [distributor_dup_dummy for distributor_dup_dummy in distributor_dummy_dup_col_arr if unq_distributor in distributor_dup_dummy]\n",
    "    distributor_dummy_df[unq_distributor_col_nme] = distributor_dummy_dup_df[unq_distributor_col_arr].sum(axis=1)\n",
    "\n",
    "distributor_dummy_col_arr = list(distributor_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(distributor_dummy_df)\n",
    "print movie_list_df[distributor_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editor\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_editor = 1\n",
    "editor_prefix = \"editor_\"\n",
    "editor_col_arr = [editor_prefix + str(i) for i in range(no_top_editor)]\n",
    "editor_arr = []\n",
    "editor_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    editor_list = filter(None, row['editor'].split(\"\\r\"))[:no_top_editor]\n",
    "    editor_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in editor_list]\n",
    "    editor_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in editor_list]\n",
    "    for editor in editor_list:\n",
    "        editor_master_list.add(editor)\n",
    "    editor_list_len = len(editor_list)\n",
    "    if editor_list_len < no_top_editor:\n",
    "        for i in range(no_top_editor):\n",
    "            if i >= editor_list_len:\n",
    "                editor_list.append(\"\") \n",
    "    editor_arr.append(editor_list)\n",
    "\n",
    "editor_arr_tp = np.transpose(editor_arr)\n",
    "\n",
    "for item in enumerate(editor_col_arr):\n",
    "    movie_list_df[item[1]] = editor_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[editor_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "editor_dummy_dup_df = pd.get_dummies(movie_list_df[editor_col_arr])\n",
    "editor_dummy_dup_col_arr = list(editor_dummy_dup_df.columns.values)\n",
    "\n",
    "print editor_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "editor_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_editor in editor_master_list:\n",
    "    unq_editor_col_nme = editor_prefix + unq_editor\n",
    "\n",
    "    unq_editor_col_arr = [editor_dup_dummy for editor_dup_dummy in editor_dummy_dup_col_arr if unq_editor in editor_dup_dummy]\n",
    "    editor_dummy_df[unq_editor_col_nme] = editor_dummy_dup_df[unq_editor_col_arr].sum(axis=1)\n",
    "\n",
    "editor_dummy_col_arr = list(editor_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(editor_dummy_df)\n",
    "print movie_list_df[editor_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_music = 1\n",
    "music_prefix = \"music_\"\n",
    "music_col_arr = [music_prefix + str(i) for i in range(no_top_music)]\n",
    "music_arr = []\n",
    "music_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    music_list = filter(None, row['music'].split(\"\\r\"))[:no_top_music]\n",
    "    music_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in music_list]\n",
    "    music_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in music_list]\n",
    "    for music in music_list:\n",
    "        music_master_list.add(music)\n",
    "    music_list_len = len(music_list)\n",
    "    if music_list_len < no_top_music:\n",
    "        for i in range(no_top_music):\n",
    "            if i >= music_list_len:\n",
    "                music_list.append(\"\") \n",
    "    music_arr.append(music_list)\n",
    "\n",
    "music_arr_tp = np.transpose(music_arr)\n",
    "\n",
    "for item in enumerate(music_col_arr):\n",
    "    movie_list_df[item[1]] = music_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[music_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music_dummy_dup_df = pd.get_dummies(movie_list_df[music_col_arr])\n",
    "music_dummy_dup_col_arr = list(music_dummy_dup_df.columns.values)\n",
    "\n",
    "print music_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_music in music_master_list:\n",
    "    unq_music_col_nme = music_prefix + unq_music\n",
    "\n",
    "    unq_music_col_arr = [music_dup_dummy for music_dup_dummy in music_dummy_dup_col_arr if unq_music in music_dup_dummy]\n",
    "    music_dummy_df[unq_music_col_nme] = music_dummy_dup_df[unq_music_col_arr].sum(axis=1)\n",
    "\n",
    "music_dummy_col_arr = list(music_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(music_dummy_df)\n",
    "print movie_list_df[music_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producer\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_producer = 4\n",
    "producer_prefix = \"producer_\"\n",
    "producer_col_arr = [producer_prefix + str(i) for i in range(no_top_producer)]\n",
    "producer_arr = []\n",
    "producer_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    producer_list = filter(None, row['producer'].split(\"\\r\"))[:no_top_producer]\n",
    "    producer_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in producer_list]\n",
    "    producer_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in producer_list]\n",
    "    for producer in producer_list:\n",
    "        producer_master_list.add(producer)\n",
    "    producer_list_len = len(producer_list)\n",
    "    if producer_list_len < no_top_producer:\n",
    "        for i in range(no_top_producer):\n",
    "            if i >= producer_list_len:\n",
    "                producer_list.append(\"\") \n",
    "    producer_arr.append(producer_list)\n",
    "\n",
    "producer_arr_tp = np.transpose(producer_arr)\n",
    "\n",
    "for item in enumerate(producer_col_arr):\n",
    "    movie_list_df[item[1]] = producer_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[producer_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "producer_dummy_dup_df = pd.get_dummies(movie_list_df[producer_col_arr])\n",
    "producer_dummy_dup_col_arr = list(producer_dummy_dup_df.columns.values)\n",
    "\n",
    "print producer_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "producer_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_producer in producer_master_list:\n",
    "    unq_producer_col_nme = producer_prefix + unq_producer\n",
    "\n",
    "    unq_producer_col_arr = [producer_dup_dummy for producer_dup_dummy in producer_dummy_dup_col_arr if unq_producer in producer_dup_dummy]\n",
    "    producer_dummy_df[unq_producer_col_nme] = producer_dummy_dup_df[unq_producer_col_arr].sum(axis=1)\n",
    "\n",
    "producer_dummy_col_arr = list(producer_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(producer_dummy_df)\n",
    "print movie_list_df[producer_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writer\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_writer = 3\n",
    "writer_prefix = \"writer_\"\n",
    "writer_col_arr = [writer_prefix + str(i) for i in range(no_top_writer)]\n",
    "writer_arr = []\n",
    "writer_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    writer_list = filter(None, row['writer'].split(\"\\r\"))[:no_top_writer]\n",
    "    writer_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in writer_list]\n",
    "    writer_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in writer_list]\n",
    "    for writer in writer_list:\n",
    "        writer_master_list.add(writer)\n",
    "    writer_list_len = len(writer_list)\n",
    "    if writer_list_len < no_top_writer:\n",
    "        for i in range(no_top_writer):\n",
    "            if i >= writer_list_len:\n",
    "                writer_list.append(\"\") \n",
    "    writer_arr.append(writer_list)\n",
    "\n",
    "writer_arr_tp = np.transpose(writer_arr)\n",
    "\n",
    "for item in enumerate(writer_col_arr):\n",
    "    movie_list_df[item[1]] = writer_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[writer_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer_dummy_dup_df = pd.get_dummies(movie_list_df[writer_col_arr])\n",
    "writer_dummy_dup_col_arr = list(writer_dummy_dup_df.columns.values)\n",
    "\n",
    "print writer_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_writer in writer_master_list:\n",
    "    unq_writer_col_nme = writer_prefix + unq_writer\n",
    "\n",
    "    unq_writer_col_arr = [writer_dup_dummy for writer_dup_dummy in writer_dummy_dup_col_arr if unq_writer in writer_dup_dummy]\n",
    "    writer_dummy_df[unq_writer_col_nme] = writer_dummy_dup_df[unq_writer_col_arr].sum(axis=1)\n",
    "\n",
    "writer_dummy_col_arr = list(writer_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(writer_dummy_df)\n",
    "print movie_list_df[writer_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Story\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_story = 1\n",
    "story_prefix = \"story_\"\n",
    "story_col_arr = [story_prefix + str(i) for i in range(no_top_story)]\n",
    "story_arr = []\n",
    "story_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    story_list = filter(None, row['story'].split(\"\\r\"))[:no_top_story]\n",
    "    story_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in story_list]\n",
    "    story_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in story_list]\n",
    "    for story in story_list:\n",
    "        story_master_list.add(story)\n",
    "    story_list_len = len(story_list)\n",
    "    if story_list_len < no_top_story:\n",
    "        for i in range(no_top_story):\n",
    "            if i >= story_list_len:\n",
    "                story_list.append(\"\") \n",
    "    story_arr.append(story_list)\n",
    "\n",
    "story_arr_tp = np.transpose(story_arr)\n",
    "\n",
    "for item in enumerate(story_col_arr):\n",
    "    movie_list_df[item[1]] = story_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[story_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "story_dummy_dup_df = pd.get_dummies(movie_list_df[story_col_arr])\n",
    "story_dummy_dup_col_arr = list(story_dummy_dup_df.columns.values)\n",
    "\n",
    "print story_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "story_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_story in story_master_list:\n",
    "    unq_story_col_nme = story_prefix + unq_story\n",
    "\n",
    "    unq_story_col_arr = [story_dup_dummy for story_dup_dummy in story_dummy_dup_col_arr if unq_story in story_dup_dummy]\n",
    "    story_dummy_df[unq_story_col_nme] = story_dummy_dup_df[unq_story_col_arr].sum(axis=1)\n",
    "\n",
    "story_dummy_col_arr = list(story_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(story_dummy_df)\n",
    "print movie_list_df[story_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studio\n",
    "1. Retain top n results\n",
    "\n",
    "2. Generate dummy variables \n",
    "\n",
    "3. Combine duplicate columns and merge in to main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_studio = 5\n",
    "studio_prefix = \"studio_\"\n",
    "studio_col_arr = [studio_prefix + str(i) for i in range(no_top_studio)]\n",
    "studio_arr = []\n",
    "studio_master_list = set()\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    studio_list = filter(None, row['studio'].split(\"\\r\"))[:no_top_studio]\n",
    "    studio_list = [re.sub('\\[[0-9]+\\]',\"\",item) for item in studio_list]\n",
    "    studio_list = [re.sub('[^A-Za-z0-9\\s]+',\"\",item) for item in studio_list]\n",
    "    for studio in studio_list:\n",
    "        studio_master_list.add(studio)\n",
    "    studio_list_len = len(studio_list)\n",
    "    if studio_list_len < no_top_studio:\n",
    "        for i in range(no_top_studio):\n",
    "            if i >= studio_list_len:\n",
    "                studio_list.append(\"\") \n",
    "    studio_arr.append(studio_list)\n",
    "\n",
    "studio_arr_tp = np.transpose(studio_arr)\n",
    "\n",
    "for item in enumerate(studio_col_arr):\n",
    "    movie_list_df[item[1]] = studio_arr_tp[item[0]]\n",
    "\n",
    "print movie_list_df[studio_col_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "studio_dummy_dup_df = pd.get_dummies(movie_list_df[studio_col_arr])\n",
    "studio_dummy_dup_col_arr = list(studio_dummy_dup_df.columns.values)\n",
    "\n",
    "print studio_dummy_dup_col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "studio_dummy_df = pd.DataFrame()\n",
    "\n",
    "for unq_studio in studio_master_list:\n",
    "    unq_studio_col_nme = studio_prefix + unq_studio\n",
    "\n",
    "    unq_studio_col_arr = [studio_dup_dummy for studio_dup_dummy in studio_dummy_dup_col_arr if unq_studio in studio_dup_dummy]\n",
    "    studio_dummy_df[unq_studio_col_nme] = studio_dummy_dup_df[unq_studio_col_arr].sum(axis=1)\n",
    "\n",
    "studio_dummy_col_arr = list(studio_dummy_df.columns.values)\n",
    "\n",
    "movie_list_df = movie_list_df.join(studio_dummy_df)\n",
    "print movie_list_df[studio_dummy_col_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompose plots into topics using Non-Negative Matrix Factorization (NNMF), Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "no_word_features = 10000\n",
    "no_topics = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df['plot'][1]\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df['plot'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_list_df['plot_clean'] = movie_list_df['plot'].replace(to_replace='\\[[0-9]+\\]', value=\" \", regex=True)\n",
    "movie_list_df['plot_clean'].replace(to_replace='[^A-Za-z0-9]+', value=\" \", inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK to remove proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df['plot_clean'][1]\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df['plot_clean'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit NNMF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize plots for NNMF using tf-idf\n",
    "Max number of features is number of words for the \"bag of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmf_tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_word_features, stop_words='english')\n",
    "nnmf_tfidf = nnmf_tfidf_vectorizer.fit_transform(movie_list_df['plot_clean'])\n",
    "nnmf_tfidf_feature_names = nnmf_tfidf_vectorizer.get_feature_names()\n",
    "print nnmf_tfidf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NNMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(nnmf_tfidf)\n",
    "nnmf_W = nnmf_model.transform(nnmf_tfidf)\n",
    "nnmf_H = nnmf_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add full list of NNMF topic scores to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_list_df['nnmf_topic_scores'] = nnmf_W.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add top n NNMF topics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_top_n_nnmf_topics = 5\n",
    "nnmf_topic_col_arr = [\"nnmf_topic_\" + str(i) for i in range(no_top_n_nnmf_topics)]\n",
    "top_n_nnmf_topic_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    top_n_nnmf_topic_arr.append(np.array(row['nnmf_topic_scores']).argsort()[-1*no_top_n_nnmf_topics:][::-1])\n",
    "\n",
    "top_n_nnmf_topic_arr = np.transpose(top_n_nnmf_topic_arr)\n",
    "    \n",
    "for topic in enumerate(nnmf_topic_col_arr):\n",
    "    movie_list_df[topic[1]] = top_n_nnmf_topic_arr[topic[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize plots for LDA using tf\n",
    "\n",
    "Max number of features is number of words for the \"bag of words\".\n",
    "\n",
    "LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_word_features, stop_words='english')\n",
    "lda_tf = lda_tf_vectorizer.fit_transform(movie_list_df['plot_clean'])\n",
    "lda_tf_feature_names = lda_tf_vectorizer.get_feature_names()\n",
    "print lda_tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(lda_tf)\n",
    "lda_W = lda_model.transform(lda_tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LDA topics to main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_list_df['lda_topic_scores'] = lda_W.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add top n LDA topics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_top_n_lda_topics = 5\n",
    "lda_topic_col_arr = [\"lda_topic_\" + str(i) for i in range(no_top_n_lda_topics)]\n",
    "top_n_lda_topic_arr = []\n",
    "for index, row in movie_list_df.iterrows():\n",
    "    top_n_lda_topic_arr.append(np.array(row['lda_topic_scores']).argsort()[-1*no_top_n_lda_topics:][::-1])\n",
    "\n",
    "top_n_lda_topic_arr = np.transpose(top_n_lda_topic_arr)\n",
    "    \n",
    "for topic in enumerate(lda_topic_col_arr):\n",
    "    movie_list_df[topic[1]] = top_n_lda_topic_arr[topic[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print movie_list_df.columns.values\n",
    "print \"---------------------------------------------------------------------------\"\n",
    "print movie_list_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display plot model results\n",
    "Will display top associated words, top movies for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_top_words = 100\n",
    "no_top_documents = 5\n",
    "\n",
    "def display_topics(H, W, feature_names, titles, plots, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print \"\\nMovie: \" + titles[doc_index]\n",
    "            print \"Plot:\\n\" + plots[doc_index] + \"\\n\"\n",
    "        print \"---------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_topics(nnmf_H, nnmf_W, nnmf_tfidf_feature_names, movie_list_df['title'], movie_list_df['plot'], no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_topics(lda_H, lda_W, lda_tf_feature_names, movie_list_df['title'], movie_list_df['plot'], no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build revenue prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create model input arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build movie feature data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use either NNMF or LDA topic as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_prediction_features = ['costs_clean', 'length_clean', 'release_week', 'release_day_of_week', 'adaptation']\n",
    "revenue_column = 'revenues_clean'\n",
    "\n",
    "#movie_prediction_features += nnmf_topic_col_arr\n",
    "movie_prediction_features += lda_topic_col_arr\n",
    "\n",
    "movie_prediction_features += actor_dummy_col_arr\n",
    "movie_prediction_features += cinematographer_dummy_col_arr\n",
    "movie_prediction_features += director_dummy_col_arr\n",
    "movie_prediction_features += distributor_dummy_col_arr\n",
    "movie_prediction_features += editor_dummy_col_arr\n",
    "movie_prediction_features += music_dummy_col_arr\n",
    "movie_prediction_features += producer_dummy_col_arr\n",
    "movie_prediction_features += writer_dummy_col_arr\n",
    "movie_prediction_features += story_dummy_col_arr\n",
    "movie_prediction_features += studio_dummy_col_arr\n",
    "\n",
    "# add revenue here for cleaning below; will be dropped after\n",
    "movie_prediction_features.append(revenue_column)\n",
    "\n",
    "movie_feature_df = movie_list_df[movie_prediction_features]\n",
    "print movie_feature_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of rows in feature dataframe before change = \" + str(len(movie_feature_df))\n",
    "\n",
    "movie_feature_df = movie_feature_df.dropna()\n",
    "\n",
    "print \"Number of rows in feature dataframe after change = \" + str(len(movie_feature_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build numpy arrays for features, revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revenue_actl = np.array(movie_feature_df[revenue_column]).flatten()\n",
    "print revenue_actl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del movie_feature_df[revenue_column]\n",
    "print movie_feature_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_feature_arr = movie_feature_df.as_matrix()\n",
    "print \"Number of rows in feature numpy array = \" + str(len(movie_feature_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(revenue_actl)\n",
    "print revenue_actl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of features: \" + str(len(movie_prediction_features))\n",
    "print movie_prediction_features[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print movie_feature_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set into training, validation, and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, training_revenue = movie_feature_arr[:600], revenue_actl[:600]\n",
    "validation_data, validation_revenue = movie_feature_arr[601:1000], revenue_actl[601:1000]\n",
    "test_data, test_revenue = movie_feature_arr[1001:], revenue_actl[1001:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetSquaredErrorLoss(revenue_actl, revenue_pred):\n",
    "    return sum((revenue_actl - revenue_pred)**2)/(1.0*len(revenue_actl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print GetSquaredErrorLoss(revenue_actl, revenue_actl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Logarithmic Error (RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetRMSLE(revenue_actl, revenue_pred):\n",
    "    return ((1.0/len(revenue_actl)) * sum((np.log(revenue_pred) - np.log(revenue_actl))**2))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print GetRMSLE(revenue_actl, revenue_actl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare predicted and actual revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComparePredictedResults(revenue_actl, revenue_pred, no_of_rows):\n",
    "\n",
    "    print \"{: >20} {: >20}\".format('Actual Revenue', 'Predicted Revenue')\n",
    "    for i in range(no_of_rows):\n",
    "        print \"{: >20} {: >20}\".format(str(revenue_actl[i]), str(revenue_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results of validation data set using training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using a variety of models using the following procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit model using \"training\" data set\n",
    "2. Predict revenues of validation data set\n",
    "3. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_validation_revenue_pred = np.round(lm.predict(validation_data))\n",
    "lm_validation_revenue_pred[lm_validation_revenue_pred < 0] = 0\n",
    "print lm_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_error = GetSquaredErrorLoss(validation_revenue, lm_validation_revenue_pred)\n",
    "print lm_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, lm_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_validation_revenue_pred = np.round(knn.predict(validation_data))\n",
    "knn_validation_revenue_pred[knn_validation_revenue_pred < 0] = 0\n",
    "print knn_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_error = GetSquaredErrorLoss(validation_revenue, knn_validation_revenue_pred)\n",
    "print knn_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, knn_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_validation_revenue_pred = np.round(dt.predict(validation_data))\n",
    "dt_validation_revenue_pred[dt_validation_revenue_pred < 0] = 0\n",
    "print dt_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_error = GetSquaredErrorLoss(validation_revenue, dt_validation_revenue_pred)\n",
    "print dt_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, dt_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfc = RandomForestClassifier(n_estimators=1000, max_features=1000)\n",
    "rnfc.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfc_validation_revenue_pred = np.round(rnfc.predict(validation_data))\n",
    "rnfc_validation_revenue_pred[rnfc_validation_revenue_pred < 0] = 0\n",
    "print rnfc_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfc_error = GetSquaredErrorLoss(validation_revenue, rnfc_validation_revenue_pred)\n",
    "print rnfc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, rnfc_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor(max_features=1000)\n",
    "dtr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtr_validation_revenue_pred = np.round(dtr.predict(validation_data))\n",
    "dtr_validation_revenue_pred[dtr_validation_revenue_pred < 0] = 0\n",
    "print dtr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtr_error = GetSquaredErrorLoss(validation_revenue, dtr_validation_revenue_pred)\n",
    "print dtr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, dtr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfr = RandomForestRegressor(n_estimators=1000, max_features=1000)\n",
    "rnfr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfr_validation_revenue_pred = np.round(rnfr.predict(validation_data))\n",
    "rnfr_validation_revenue_pred[rnfr_validation_revenue_pred < 0] = 0\n",
    "print rnfr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnfr_error = GetSquaredErrorLoss(validation_revenue, rnfr_validation_revenue_pred)\n",
    "print rnfr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, rnfr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adbr = AdaBoostRegressor(n_estimators=1000)\n",
    "adbr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adbr_validation_revenue_pred = np.round(adbr.predict(validation_data))\n",
    "adbr_validation_revenue_pred[adbr_validation_revenue_pred < 0] = 0\n",
    "print adbr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adbr_error = GetSquaredErrorLoss(validation_revenue, adbr_validation_revenue_pred)\n",
    "print adbr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, adbr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bgr = BaggingRegressor(n_estimators = 1000)\n",
    "bgr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bgr_validation_revenue_pred = np.round(bgr.predict(validation_data))\n",
    "bgr_validation_revenue_pred[bgr_validation_revenue_pred < 0] = 0\n",
    "print bgr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bgr_error = GetSquaredErrorLoss(validation_revenue, bgr_validation_revenue_pred)\n",
    "print bgr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, bgr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 1000)\n",
    "gbr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr_validation_revenue_pred = np.round(gbr.predict(validation_data))\n",
    "gbr_validation_revenue_pred[gbr_validation_revenue_pred < 0] = 0\n",
    "print gbr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbr_error = GetSquaredErrorLoss(validation_revenue, gbr_validation_revenue_pred)\n",
    "print gbr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, gbr_validation_revenue_pred, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdr = SGDRegressor()\n",
    "sgdr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdr_validation_revenue_pred = np.round(sgdr.predict(validation_data))\n",
    "sgdr_validation_revenue_pred[sgdr_validation_revenue_pred < 0] = 0\n",
    "print sgdr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgdr_error = GetSquaredErrorLoss(validation_revenue, sgdr_validation_revenue_pred)\n",
    "print sgdr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, sgdr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmr = SVR()\n",
    "svmr.fit(training_data, training_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmr_validation_revenue_pred = np.round(svmr.predict(validation_data))\n",
    "svmr_validation_revenue_pred[svmr_validation_revenue_pred < 0] = 0\n",
    "print svmr_validation_revenue_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmr_error = GetSquaredErrorLoss(validation_revenue, svmr_validation_revenue_pred)\n",
    "print svmr_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ComparePredictedResults(validation_revenue, svmr_validation_revenue_pred, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot comparison of actual vs predicted revenues for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))    \n",
    "\n",
    "#plt.plot(range(len(validation_revenue)), validation_revenue, 'o', ms=6, color='blue', label='Actual')\n",
    "#plt.plot(range(len(gbr_validation_revenue_pred)), gbr_validation_revenue_pred, 'o', ms=6, color='red', label='Predicted')\n",
    "#plt.plot(range(len(validation_revenue)), abs(validation_revenue - gbr_validation_revenue_pred), 'o', ms=6, color='blue', label='Actual')\n",
    "data = abs(validation_revenue - gbr_validation_revenue_pred)\n",
    "\n",
    "plt.hist(data, bins=100)\n",
    "\n",
    "#plt.xlabel('Movie')\n",
    "#plt.ylabel('Revenue')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('residuals.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get associated topic scores for input plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get prediction array for input movie  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted revenue for input movie "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
